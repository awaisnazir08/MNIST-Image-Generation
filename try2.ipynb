{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images: (60000, 28, 28)\n",
      "train_labels: (60000,)\n",
      "test_images:  (10000, 28, 28)\n",
      "test_labels:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "#loading the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "#printing the shapes of the vectors \n",
    "print('train_images: ' + str(train_images.shape))\n",
    "print('train_labels: ' + str(train_labels.shape))\n",
    "print('test_images:  '  + str(test_images.shape))\n",
    "print('test_labels:  '  + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training and test images\n",
    "train_images = train_images.reshape((60000, 784))  # Flatten images (60000, 28, 28) -> (60000, 784)\n",
    "train_images = train_images.astype('float32') / 255  # Normalize pixel values\n",
    "\n",
    "test_images = test_images.reshape((10000, 784))  # Flatten images (10000, 28, 28) -> (10000, 784)\n",
    "\n",
    "test_images = test_images.astype('float32') / 255  # Normalize pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images: (60000, 784)\n",
      "train_labels: (60000,)\n",
      "test_images:  (10000, 784)\n",
      "test_labels:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "#printing the shapes of the vectors \n",
    "print('train_images: ' + str(train_images.shape))\n",
    "print('train_labels: ' + str(train_labels.shape))\n",
    "print('test_images:  '  + str(test_images.shape))\n",
    "print('test_labels:  '  + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels: (60000, 1)\n",
      "test_labels:  (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_labels=train_labels.reshape(60000,1)\n",
    "test_labels = test_labels.reshape(10000,1)\n",
    "print('train_labels: ' + str(train_labels.shape))\n",
    "print('test_labels:  '  + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample 28x28 image (you would replace this with your image data)\n",
    "image_flattened = train_images[34]  # Example random image\n",
    "\n",
    "# Display the original image\n",
    "# plt.imshow(image_flattened, cmap='gray')\n",
    "# plt.title('Original 28x28 Image')\n",
    "# plt.show()\n",
    "\n",
    "# Flatten the image into a 784x1 vector\n",
    "# image_flattened = image_28x28.reshape(784, 1)\n",
    "\n",
    "# Reconstruct the image back into a 28x28 matrix\n",
    "image_reconstructed = image_flattened.reshape(28, 28)\n",
    "\n",
    "# Display the reconstructed image\n",
    "plt.imshow(image_reconstructed, cmap='gray')\n",
    "plt.title('Reconstructed 28x28 Image')\n",
    "plt.show()\n",
    "\n",
    "# Check if the reconstructed image matches the original\n",
    "# You can calculate the mean squared error (MSE) to verify\n",
    "# mse = np.mean((image_flattened - image_reconstructed)**2)\n",
    "# print(f\"Mean Squared Error (MSE) between original and reconstructed image: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "601 183\n"
     ]
    }
   ],
   "source": [
    "print(min(image_flattened))\n",
    "print(max(image_flattened))\n",
    "b0 = 0\n",
    "b1 = 0\n",
    "for element in image_flattened:\n",
    "    if element == 0:\n",
    "        b0 += 1\n",
    "    else:\n",
    "        b1 += 1\n",
    "print(b0,b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5]\n",
      " [0]\n",
      " [4]\n",
      " ...\n",
      " [5]\n",
      " [6]\n",
      " [8]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input layer (n_x): 1\n",
      "Size of hidden layer (n_h): 128\n",
      "Size of output layer (n_y): 784\n"
     ]
    }
   ],
   "source": [
    "input_size = 1 # 28 * 28 = 784\n",
    "\n",
    "# Size of the hidden layer (choose an appropriate value based on task complexity)\n",
    "hidden_size = 128  # Example: Using 128 neurons in the hidden layer\n",
    "\n",
    "# Size of the output layer (number of output units)\n",
    "output_size = 784  # 28 * 28 = 784 (output image size)\n",
    "\n",
    "# Print the defined variables\n",
    "print(f\"Size of input layer (n_x): {input_size}\")\n",
    "print(f\"Size of hidden layer (n_h): {hidden_size}\")\n",
    "print(f\"Size of output layer (n_y): {output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01764052]\n",
      " [ 0.00400157]\n",
      " [ 0.00978738]\n",
      " [ 0.02240893]\n",
      " [ 0.01867558]\n",
      " [-0.00977278]\n",
      " [ 0.00950088]\n",
      " [-0.00151357]\n",
      " [-0.00103219]\n",
      " [ 0.00410599]\n",
      " [ 0.00144044]\n",
      " [ 0.01454274]\n",
      " [ 0.00761038]\n",
      " [ 0.00121675]\n",
      " [ 0.00443863]\n",
      " [ 0.00333674]\n",
      " [ 0.01494079]\n",
      " [-0.00205158]\n",
      " [ 0.00313068]\n",
      " [-0.00854096]\n",
      " [-0.0255299 ]\n",
      " [ 0.00653619]\n",
      " [ 0.00864436]\n",
      " [-0.00742165]\n",
      " [ 0.02269755]\n",
      " [-0.01454366]\n",
      " [ 0.00045759]\n",
      " [-0.00187184]\n",
      " [ 0.01532779]\n",
      " [ 0.01469359]\n",
      " [ 0.00154947]\n",
      " [ 0.00378163]\n",
      " [-0.00887786]\n",
      " [-0.01980796]\n",
      " [-0.00347912]\n",
      " [ 0.00156349]\n",
      " [ 0.01230291]\n",
      " [ 0.0120238 ]\n",
      " [-0.00387327]\n",
      " [-0.00302303]\n",
      " [-0.01048553]\n",
      " [-0.01420018]\n",
      " [-0.0170627 ]\n",
      " [ 0.01950775]\n",
      " [-0.00509652]\n",
      " [-0.00438074]\n",
      " [-0.01252795]\n",
      " [ 0.0077749 ]\n",
      " [-0.01613898]\n",
      " [-0.0021274 ]\n",
      " [-0.00895467]\n",
      " [ 0.00386902]\n",
      " [-0.00510805]\n",
      " [-0.01180632]\n",
      " [-0.00028182]\n",
      " [ 0.00428332]\n",
      " [ 0.00066517]\n",
      " [ 0.00302472]\n",
      " [-0.00634322]\n",
      " [-0.00362741]\n",
      " [-0.0067246 ]\n",
      " [-0.00359553]\n",
      " [-0.00813146]\n",
      " [-0.01726283]\n",
      " [ 0.00177426]\n",
      " [-0.00401781]\n",
      " [-0.01630198]\n",
      " [ 0.00462782]\n",
      " [-0.00907298]\n",
      " [ 0.00051945]\n",
      " [ 0.00729091]\n",
      " [ 0.00128983]\n",
      " [ 0.01139401]\n",
      " [-0.01234826]\n",
      " [ 0.00402342]\n",
      " [-0.0068481 ]\n",
      " [-0.00870797]\n",
      " [-0.0057885 ]\n",
      " [-0.00311553]\n",
      " [ 0.00056165]\n",
      " [-0.0116515 ]\n",
      " [ 0.00900826]\n",
      " [ 0.00465662]\n",
      " [-0.01536244]\n",
      " [ 0.01488252]\n",
      " [ 0.01895889]\n",
      " [ 0.0117878 ]\n",
      " [-0.00179925]\n",
      " [-0.01070753]\n",
      " [ 0.01054452]\n",
      " [-0.00403177]\n",
      " [ 0.01222445]\n",
      " [ 0.00208275]\n",
      " [ 0.00976639]\n",
      " [ 0.00356366]\n",
      " [ 0.00706573]\n",
      " [ 0.000105  ]\n",
      " [ 0.0178587 ]\n",
      " [ 0.00126912]\n",
      " [ 0.00401989]\n",
      " [ 0.01883151]\n",
      " [-0.01347759]\n",
      " [-0.01270485]\n",
      " [ 0.00969397]\n",
      " [-0.01173123]\n",
      " [ 0.01943621]\n",
      " [-0.00413619]\n",
      " [-0.00747455]\n",
      " [ 0.01922942]\n",
      " [ 0.01480515]\n",
      " [ 0.01867559]\n",
      " [ 0.00906045]\n",
      " [-0.00861226]\n",
      " [ 0.01910065]\n",
      " [-0.00268003]\n",
      " [ 0.00802456]\n",
      " [ 0.00947252]\n",
      " [-0.0015501 ]\n",
      " [ 0.00614079]\n",
      " [ 0.00922207]\n",
      " [ 0.00376426]\n",
      " [-0.01099401]\n",
      " [ 0.00298238]\n",
      " [ 0.01326386]\n",
      " [-0.00694568]\n",
      " [-0.00149635]\n",
      " [-0.00435154]\n",
      " [ 0.01849264]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.00672295  0.00407462 -0.00769916 ... -0.00319328  0.00691539\n",
      "   0.00694749]\n",
      " [-0.00725597 -0.01383364 -0.01582938 ...  0.0038728  -0.02255564\n",
      "  -0.01022507]\n",
      " [ 0.00038631 -0.01656715 -0.00985511 ...  0.01301428  0.0089526\n",
      "   0.01374964]\n",
      " ...\n",
      " [-0.0002911   0.01645325 -0.00547952 ...  0.01639051  0.01783903\n",
      "   0.00510913]\n",
      " [-0.0046729   0.01122228  0.0098885  ... -0.00455941  0.01044034\n",
      "   0.01277395]\n",
      " [-0.00556597  0.01602401  0.00266189 ...  0.00519858 -0.00415621\n",
      "  -0.01241349]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 128)\n"
     ]
    }
   ],
   "source": [
    "print(parameters['W2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_labels))\n",
    "c = parameters['b1']\n",
    "print(type(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    # retrieve the parameters\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # compute the activation of the hidden layer\n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    # print(Z1.shape)\n",
    "    A1 = sigmoid(Z1)\n",
    "    # print(A1.shape)\n",
    "    # compute the activation of the output layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    # print(Z2.shape)\n",
    "    # print(A2.shape)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2, cache = forward_propagation(train_labels, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(A2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(784, 60000)\n",
      "(784, 128)\n",
      "(128, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(A2.shape)\n",
    "a1 = cache['A1']\n",
    "print(parameters['W2'].shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross-entropy loss function\n",
    "def binary_cross_entropy_loss(A2, y):\n",
    "    epsilon = 1e-8\n",
    "    m = y.shape[0]\n",
    "    # loss = -(1/m) * np.sum(y*np.log(A2.T) + (1-y)*np.log(1-A2.T))\n",
    "    loss = -(1/m) * np.sum(y * np.log(A2.T + epsilon) + (1 - y) * np.log(1 - A2.T + epsilon))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(cache['A2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 128)\n"
     ]
    }
   ],
   "source": [
    "print(parameters['W2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    #(≈ 2 lines of code)\n",
    "    # W1 = ...\n",
    "    # W2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    #(≈ 2 lines of code)\n",
    "    # A1 = ...\n",
    "    # A2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
    "    # dZ2 = ...\n",
    "    # dW2 = ...\n",
    "    # db2 = ...\n",
    "    # dZ1 = ...\n",
    "    # dW1 = ...\n",
    "    # db1 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dZ2 = A2 - Y.T\n",
    "    dW2 = np.dot(dZ2,A1.T)\n",
    "    dW2 = dW2/m\n",
    "    db2 = (np.sum(dZ2,axis = 1, keepdims = True))/m\n",
    "    print('db2 shape',db2.shape)\n",
    "    print('w2 shape ', W2.shape)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (A1 * (1 - A1))\n",
    "    dW1 = np.dot(dZ1,X)/m\n",
    "    db1 = np.sum(dZ1, axis = 1, keepdims=True)/m\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameters\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    # retrieve the gradients\n",
    "    dW1 = gradients[\"dW1\"]\n",
    "    db1 = gradients[\"db1\"]\n",
    "    dW2 = gradients[\"dW2\"]\n",
    "    db2 = gradients[\"db2\"]\n",
    "    \n",
    "    # retrieve the weights and biases\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # update the weights and biases\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 0: loss = 544.1471466679554\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 1: loss = 1876.9329298979685\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 2: loss = 3225.8770217593988\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 3: loss = 2516.908300091979\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 4: loss = 2611.492409402964\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 5: loss = 2596.8146941137757\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 6: loss = 2568.3974319716835\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 7: loss = 2495.0484081331283\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 8: loss = 2565.1190767696617\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 9: loss = 2480.0985346777943\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 10: loss = 2488.427167766791\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 11: loss = 2467.850935203095\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 12: loss = 2529.405288912907\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 13: loss = 2492.433444410405\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 14: loss = 2401.9699808317882\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 15: loss = 2595.4400782332223\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 16: loss = 2508.813583877037\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 17: loss = 2458.8045502155383\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 18: loss = 2540.7466172619456\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 19: loss = 2480.0875764006164\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 20: loss = 2444.504671069438\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 21: loss = 2525.796994733697\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 22: loss = 2494.851237652499\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 23: loss = 2403.680171392948\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 24: loss = 2543.31264630453\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 25: loss = 2525.6704668912885\n",
      "db2 shape (784, 1)\n",
      "w2 shape  (784, 128)\n",
      "iteration 26: loss = 2436.874503096335\n"
     ]
    }
   ],
   "source": [
    "# train the neural network\n",
    "def train(X, y, num_iterations, learning_rate):\n",
    "    # initialize the weights and biases\n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # forward propagation\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = binary_cross_entropy_loss(A2, y)\n",
    "        \n",
    "        # backward propagation\n",
    "        gradients = backward_propagation(parameters, cache, X, y)\n",
    "        \n",
    "        # update the parameters\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        # if i % 1000:\n",
    "        print(f\"iteration {i}: loss = {loss}\")\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = train(train_labels, train_images, num_iterations=10000, learning_rate=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
