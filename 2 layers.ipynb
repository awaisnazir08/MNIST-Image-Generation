{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images: (60000, 28, 28)\n",
      "train_labels: (60000,)\n",
      "test_images:  (10000, 28, 28)\n",
      "test_labels:  (10000,)\n",
      "train_images_subset: (5000, 28, 28)\n",
      "train_labels_subset: (5000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "#loading the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "#printing the shapes of the vectors \n",
    "print('train_images: ' + str(train_images.shape))\n",
    "print('train_labels: ' + str(train_labels.shape))\n",
    "print('test_images:  '  + str(test_images.shape))\n",
    "print('test_labels:  '  + str(test_labels.shape))\n",
    "num_train_samples = 5000\n",
    "train_images = train_images[:num_train_samples]\n",
    "train_labels = train_labels[:num_train_samples]\n",
    "\n",
    "# Printing the shapes of the subsets\n",
    "print('train_images_subset: ' + str(train_images.shape))\n",
    "print('train_labels_subset: ' + str(train_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training and test images\n",
    "train_images = train_images.reshape((5000, 784))  \n",
    "train_images = train_images.astype('float32') / 255  # Normalize pixel values\n",
    "\n",
    "test_images = test_images.reshape((10000, 784))  # Flatten images (10000, 28, 28) -> (10000, 784)\n",
    "\n",
    "test_images = test_images.astype('float32') / 255  # Normalize pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images: (5000, 784)\n",
      "train_labels: (5000,)\n",
      "test_images:  (10000, 784)\n",
      "test_labels:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "#printing the shapes of the vectors \n",
    "print('train_images: ' + str(train_images.shape))\n",
    "print('train_labels: ' + str(train_labels.shape))\n",
    "print('test_images:  '  + str(test_images.shape))\n",
    "print('test_labels:  '  + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels: (5000, 1)\n",
      "test_labels:  (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "train_labels=train_labels.reshape(5000,1)\n",
    "test_labels = test_labels.reshape(10000,1)\n",
    "print('train_labels: ' + str(train_labels.shape))\n",
    "print('test_labels:  '  + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample 28x28 image (you would replace this with your image data)\n",
    "image_flattened = train_images[34]\n",
    "\n",
    "# Reconstruct the image back into a 28x28 matrix\n",
    "image_reconstructed = image_flattened.reshape(28, 28)\n",
    "\n",
    "# Display the reconstructed image\n",
    "plt.imshow(image_reconstructed, cmap='gray')\n",
    "plt.title('Reconstructed 28x28 Image')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "601 183\n"
     ]
    }
   ],
   "source": [
    "print(min(image_flattened))\n",
    "print(max(image_flattened))\n",
    "b0 = 0\n",
    "b1 = 0\n",
    "for element in image_flattened:\n",
    "    if element == 0:\n",
    "        b0 += 1\n",
    "    else:\n",
    "        b1 += 1\n",
    "print(b0,b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5]\n",
      " [0]\n",
      " [4]\n",
      " ...\n",
      " [5]\n",
      " [6]\n",
      " [8]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input layer: 1\n",
      "Size of hidden layer: 128\n",
      "Size of hidden layer 2: 64\n",
      "Size of output layer: 784\n"
     ]
    }
   ],
   "source": [
    "input_size = 1 # 28 * 28 = 784\n",
    "\n",
    "hidden_size = 128  # Example: Using 128 neurons in the hidden layer\n",
    "hidden_size2 = 64\n",
    "# Size of the output layer (number of output units)\n",
    "output_size = 784  # 28 * 28 = 784 (output image size)\n",
    "\n",
    "# Print the defined variables\n",
    "print(f\"Size of input layer: {input_size}\")\n",
    "print(f\"Size of hidden layer: {hidden_size}\")\n",
    "print(f\"Size of hidden layer 2: {hidden_size2}\")\n",
    "print(f\"Size of output layer: {output_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(0)\n",
    "    W1 = np.zeros((hidden_size, input_size))\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.zeros((hidden_size2, hidden_size))\n",
    "    b2 = np.zeros((hidden_size2, 1))\n",
    "    W3 = np.zeros((output_size, hidden_size2))\n",
    "    b3 = np.zeros((output_size, 1))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, 'b3': b3}\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 128)\n"
     ]
    }
   ],
   "source": [
    "print(parameters['W2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_loss(A2, y):\n",
    "    m = y.shape[0]\n",
    "    loss = (1 / m) * np.sum(np.square(A2.T - y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_labels))\n",
    "c = parameters['b1']\n",
    "print(type(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    # retrieve the parameters\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # compute the activation of the hidden layer\n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    # print(Z1.shape)\n",
    "    A1 = relu(Z1)\n",
    "    # print(A1.shape)\n",
    "    # compute the activation of the output layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    # print(Z2.shape)\n",
    "    # print(A2.shape)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2, 'Z3': Z3, 'A3':A3}\n",
    "    \n",
    "    return A3, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3, cache = forward_propagation(train_labels, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(A3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784)\n",
      "(784, 5000)\n",
      "(784, 128)\n",
      "(128, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(A3.shape)\n",
    "a1 = cache['A1']\n",
    "print(parameters['W2'].shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross-entropy loss function\n",
    "def binary_cross_entropy_loss(A3, y):\n",
    "    epsilon = 1e-8\n",
    "    m = y.shape[0]\n",
    "    # loss = -(1/m) * np.sum(y*np.log(A2.T) + (1-y)*np.log(1-A2.T))\n",
    "    loss = -(1/m) * np.sum(y * np.log(A3.T + epsilon) + (1 - y) * np.log(1 - A3.T + epsilon))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(cache['A2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 128)\n"
     ]
    }
   ],
   "source": [
    "print(parameters['W2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a neural network with two hidden layers.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\", \"A2\", \"Z3\", \"A3\"\n",
    "    X -- input data of shape (input_size, m)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    A3 = cache['A3']\n",
    "    \n",
    "    # Compute gradients\n",
    "    dZ3 = A3 - Y.T\n",
    "    dW3 = np.dot(dZ3, A2.T) / m\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dZ2 = np.dot(W3.T, dZ3) * (A2 * (1 - A2))  # derivative of sigmoid\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dZ1 = np.dot(W2.T, dZ2) * (A1 * (1 - A1))  # derivative of sigmoid\n",
    "    dW1 = np.dot(dZ1, X) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    \n",
    "    # Store gradients in a dictionary\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\": dW3,\n",
    "             \"db3\": db3}\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = backward_propagation(parameters, cache, train_labels, train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1)\n",
      "(784, 1)\n"
     ]
    }
   ],
   "source": [
    "print(gradients['db1'].shape)\n",
    "print(gradients['db2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent for a neural network with two hidden layers.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters (W1, b1, W2, b2, W3, b3)\n",
    "    gradients -- python dictionary containing your gradients (dW1, db1, dW2, db2, dW3, db3)\n",
    "    learning_rate -- the learning rate used in the update rule\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    \"\"\"\n",
    "    # Retrieve gradients\n",
    "    dW1 = gradients[\"dW1\"]\n",
    "    db1 = gradients[\"db1\"]\n",
    "    dW2 = gradients[\"dW2\"]\n",
    "    db2 = gradients[\"db2\"]\n",
    "    dW3 = gradients[\"dW3\"]\n",
    "    db3 = gradients[\"db3\"]\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    \n",
    "    # Store updated parameters in a dictionary\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss = 543.4273740751004\n",
      "iteration 1: loss = 234.14634276691447\n",
      "iteration 2: loss = 224.30754383658294\n",
      "iteration 3: loss = 219.55891345590484\n",
      "iteration 4: loss = 216.71328286565037\n",
      "iteration 5: loss = 214.8188000495359\n",
      "iteration 6: loss = 213.46898942386133\n",
      "iteration 7: loss = 212.46000646328807\n",
      "iteration 8: loss = 211.67827552883477\n",
      "iteration 9: loss = 211.05551185106688\n",
      "iteration 10: loss = 210.5482092441369\n",
      "iteration 11: loss = 210.12734911530117\n",
      "iteration 12: loss = 209.77284290686475\n",
      "iteration 13: loss = 209.4703488572896\n",
      "iteration 14: loss = 209.20935860837548\n",
      "iteration 15: loss = 208.9819997583188\n",
      "iteration 16: loss = 208.78226034140593\n",
      "iteration 17: loss = 208.6054715227448\n",
      "iteration 18: loss = 208.4479535518982\n",
      "iteration 19: loss = 208.30676791690277\n",
      "iteration 20: loss = 208.17954033550663\n",
      "iteration 21: loss = 208.06433205970094\n",
      "iteration 22: loss = 207.95954479433163\n",
      "iteration 23: loss = 207.8638494256869\n",
      "iteration 24: loss = 207.77613189119165\n",
      "iteration 25: loss = 207.69545157245938\n",
      "iteration 26: loss = 207.621008961915\n",
      "iteration 27: loss = 207.55212028177928\n",
      "iteration 28: loss = 207.4881973746865\n",
      "iteration 29: loss = 207.42873163362205\n",
      "iteration 30: loss = 207.37328105709773\n",
      "iteration 31: loss = 207.32145974420877\n",
      "iteration 32: loss = 207.2729293105502\n",
      "iteration 33: loss = 207.22739182826024\n",
      "iteration 34: loss = 207.18458398428297\n",
      "iteration 35: loss = 207.14427221904805\n",
      "iteration 36: loss = 207.10624865929785\n",
      "iteration 37: loss = 207.07032769808723\n",
      "iteration 38: loss = 207.03634310522688\n",
      "iteration 39: loss = 207.0041455748411\n",
      "iteration 40: loss = 206.97360063499016\n",
      "iteration 41: loss = 206.94458685865283\n",
      "iteration 42: loss = 206.91699432669452\n",
      "iteration 43: loss = 206.89072330247154\n",
      "iteration 44: loss = 206.86568308491675\n",
      "iteration 45: loss = 206.8417910127536\n",
      "iteration 46: loss = 206.8189715971592\n",
      "iteration 47: loss = 206.7971557639988\n",
      "iteration 48: loss = 206.77628018985465\n",
      "iteration 49: loss = 206.75628671860687\n",
      "iteration 50: loss = 206.7371218474211\n",
      "iteration 51: loss = 206.71873627271435\n",
      "iteration 52: loss = 206.70108448811047\n",
      "iteration 53: loss = 206.68412442758472\n",
      "iteration 54: loss = 206.6678171479908\n",
      "iteration 55: loss = 206.6521265460046\n",
      "iteration 56: loss = 206.63701910521903\n",
      "iteration 57: loss = 206.62246366971024\n",
      "iteration 58: loss = 206.60843124091224\n",
      "iteration 59: loss = 206.59489479504856\n",
      "iteration 60: loss = 206.58182911873882\n",
      "iteration 61: loss = 206.56921066071055\n",
      "iteration 62: loss = 206.55701739780758\n",
      "iteration 63: loss = 206.54522871371455\n",
      "iteration 64: loss = 206.53382528901437\n",
      "iteration 65: loss = 206.52278900136733\n",
      "iteration 66: loss = 206.51210283473793\n",
      "iteration 67: loss = 206.50175079673497\n",
      "iteration 68: loss = 206.49171784322735\n",
      "iteration 69: loss = 206.4819898095046\n",
      "iteration 70: loss = 206.47255334732822\n",
      "iteration 71: loss = 206.46339586729565\n",
      "iteration 72: loss = 206.454505486002\n",
      "iteration 73: loss = 206.44587097754052\n",
      "iteration 74: loss = 206.43748172893476\n",
      "iteration 75: loss = 206.42932769913105\n",
      "iteration 76: loss = 206.42139938123262\n",
      "iteration 77: loss = 206.4136877676735\n",
      "iteration 78: loss = 206.40618431807138\n",
      "iteration 79: loss = 206.3988809295264\n",
      "iteration 80: loss = 206.39176990914666\n",
      "iteration 81: loss = 206.38484394861058\n",
      "iteration 82: loss = 206.37809610059398\n",
      "iteration 83: loss = 206.3715197569023\n",
      "iteration 84: loss = 206.36510862816806\n",
      "iteration 85: loss = 206.35885672498227\n",
      "iteration 86: loss = 206.3527583403469\n",
      "iteration 87: loss = 206.34680803333868\n",
      "iteration 88: loss = 206.34100061388853\n",
      "iteration 89: loss = 206.33533112859263\n",
      "iteration 90: loss = 206.32979484746735\n",
      "iteration 91: loss = 206.32438725158522\n",
      "iteration 92: loss = 206.31910402151465\n",
      "iteration 93: loss = 206.31394102651188\n",
      "iteration 94: loss = 206.3088943144029\n",
      "iteration 95: loss = 206.3039601021069\n",
      "iteration 96: loss = 206.29913476675506\n",
      "iteration 97: loss = 206.29441483736045\n",
      "iteration 98: loss = 206.28979698699965\n",
      "iteration 99: loss = 206.285278025471\n",
      "iteration 100: loss = 206.28085489239453\n",
      "iteration 101: loss = 206.27652465072327\n",
      "iteration 102: loss = 206.2722844806418\n",
      "iteration 103: loss = 206.26813167381493\n",
      "iteration 104: loss = 206.26406362797712\n",
      "iteration 105: loss = 206.26007784182772\n",
      "iteration 106: loss = 206.25617191022056\n",
      "iteration 107: loss = 206.25234351962035\n",
      "iteration 108: loss = 206.24859044381833\n",
      "iteration 109: loss = 206.24491053987902\n",
      "iteration 110: loss = 206.24130174431573\n",
      "iteration 111: loss = 206.2377620694697\n",
      "iteration 112: loss = 206.23428960008457\n",
      "iteration 113: loss = 206.2308824900681\n",
      "iteration 114: loss = 206.22753895941923\n",
      "iteration 115: loss = 206.2242572913228\n",
      "iteration 116: loss = 206.22103582939002\n",
      "iteration 117: loss = 206.2178729750466\n",
      "iteration 118: loss = 206.2147671850506\n",
      "iteration 119: loss = 206.21171696913754\n",
      "iteration 120: loss = 206.20872088778623\n",
      "iteration 121: loss = 206.2057775500931\n",
      "iteration 122: loss = 206.20288561175374\n",
      "iteration 123: loss = 206.20004377314402\n",
      "iteration 124: loss = 206.19725077749243\n",
      "iteration 125: loss = 206.19450540914403\n",
      "iteration 126: loss = 206.19180649190665\n",
      "iteration 127: loss = 206.18915288747294\n",
      "iteration 128: loss = 206.18654349392196\n",
      "iteration 129: loss = 206.1839772442891\n",
      "iteration 130: loss = 206.18145310519952\n",
      "iteration 131: loss = 206.17897007556974\n",
      "iteration 132: loss = 206.1765271853654\n",
      "iteration 133: loss = 206.1741234944176\n",
      "iteration 134: loss = 206.17175809129103\n",
      "iteration 135: loss = 206.16943009220327\n",
      "iteration 136: loss = 206.16713863999396\n",
      "iteration 137: loss = 206.16488290313694\n",
      "iteration 138: loss = 206.16266207479768\n",
      "iteration 139: loss = 206.1604753719315\n",
      "iteration 140: loss = 206.1583220344186\n",
      "iteration 141: loss = 206.15620132424132\n",
      "iteration 142: loss = 206.1541125246905\n",
      "iteration 143: loss = 206.15205493960985\n",
      "iteration 144: loss = 206.15002789267007\n",
      "iteration 145: loss = 206.14803072667493\n",
      "iteration 146: loss = 206.14606280289297\n",
      "iteration 147: loss = 206.144123500422\n",
      "iteration 148: loss = 206.142212215573\n",
      "iteration 149: loss = 206.14032836128644\n",
      "iteration 150: loss = 206.13847136656577\n",
      "iteration 151: loss = 206.13664067593746\n",
      "iteration 152: loss = 206.1348357489313\n",
      "iteration 153: loss = 206.13305605958206\n",
      "iteration 154: loss = 206.13130109594923\n",
      "iteration 155: loss = 206.12957035965766\n",
      "iteration 156: loss = 206.1278633654539\n",
      "iteration 157: loss = 206.126179640783\n",
      "iteration 158: loss = 206.12451872537602\n",
      "iteration 159: loss = 206.1228801708601\n",
      "iteration 160: loss = 206.1212635403776\n",
      "iteration 161: loss = 206.11966840822322\n",
      "iteration 162: loss = 206.11809435949178\n",
      "iteration 163: loss = 206.1165409897436\n",
      "iteration 164: loss = 206.1150079046764\n",
      "iteration 165: loss = 206.11349471981424\n",
      "iteration 166: loss = 206.1120010602055\n",
      "iteration 167: loss = 206.11052656013197\n",
      "iteration 168: loss = 206.109070862829\n",
      "iteration 169: loss = 206.10763362021555\n",
      "iteration 170: loss = 206.10621449263394\n",
      "iteration 171: loss = 206.1048131485985\n",
      "iteration 172: loss = 206.10342926455368\n",
      "iteration 173: loss = 206.10206252463973\n",
      "iteration 174: loss = 206.10071262046813\n",
      "iteration 175: loss = 206.09937925090352\n",
      "iteration 176: loss = 206.09806212185254\n",
      "iteration 177: loss = 206.0967609460627\n",
      "iteration 178: loss = 206.09547544292445\n",
      "iteration 179: loss = 206.0942053382843\n",
      "iteration 180: loss = 206.09295036425863\n",
      "iteration 181: loss = 206.09171025906002\n",
      "iteration 182: loss = 206.09048476682463\n",
      "iteration 183: loss = 206.08927363744678\n",
      "iteration 184: loss = 206.08807662642073\n",
      "iteration 185: loss = 206.0868934946846\n",
      "iteration 186: loss = 206.0857240084727\n",
      "iteration 187: loss = 206.0845679391682\n",
      "iteration 188: loss = 206.08342506316654\n",
      "iteration 189: loss = 206.08229516173773\n",
      "iteration 190: loss = 206.0811780208962\n",
      "iteration 191: loss = 206.08007343127395\n",
      "iteration 192: loss = 206.07898118799784\n",
      "iteration 193: loss = 206.07790109057015\n",
      "iteration 194: loss = 206.0768329427532\n",
      "iteration 195: loss = 206.07577655245902\n",
      "iteration 196: loss = 206.07473173164004\n",
      "iteration 197: loss = 206.07369829618438\n",
      "iteration 198: loss = 206.07267606581502\n",
      "iteration 199: loss = 206.07166486398984\n",
      "iteration 200: loss = 206.07066451780818\n",
      "iteration 201: loss = 206.06967485791702\n",
      "iteration 202: loss = 206.0686957184207\n",
      "iteration 203: loss = 206.06772693679534\n",
      "iteration 204: loss = 206.06676835380338\n",
      "iteration 205: loss = 206.06581981341193\n",
      "iteration 206: loss = 206.06488116271265\n",
      "iteration 207: loss = 206.06395225184545\n",
      "iteration 208: loss = 206.0630329339231\n",
      "iteration 209: loss = 206.0621230649583\n",
      "iteration 210: loss = 206.06122250379332\n",
      "iteration 211: loss = 206.06033111203183\n",
      "iteration 212: loss = 206.05944875397137\n",
      "iteration 213: loss = 206.05857529653895\n",
      "iteration 214: loss = 206.05771060922865\n",
      "iteration 215: loss = 206.05685456404004\n",
      "iteration 216: loss = 206.0560070354193\n",
      "iteration 217: loss = 206.05516790020096\n",
      "iteration 218: loss = 206.05433703755202\n",
      "iteration 219: loss = 206.05351432891797\n",
      "iteration 220: loss = 206.052699657969\n",
      "iteration 221: loss = 206.05189291054842\n",
      "iteration 222: loss = 206.05109397462434\n",
      "iteration 223: loss = 206.050302740238\n",
      "iteration 224: loss = 206.04951909945896\n",
      "iteration 225: loss = 206.04874294633723\n",
      "iteration 226: loss = 206.0479741768595\n",
      "iteration 227: loss = 206.0472126889049\n",
      "iteration 228: loss = 206.04645838220299\n",
      "iteration 229: loss = 206.04571115829165\n",
      "iteration 230: loss = 206.0449709204765\n",
      "iteration 231: loss = 206.0442375737942\n",
      "iteration 232: loss = 206.04351102497083\n",
      "iteration 233: loss = 206.04279118238736\n",
      "iteration 234: loss = 206.04207795604157\n",
      "iteration 235: loss = 206.04137125751515\n",
      "iteration 236: loss = 206.04067099993605\n",
      "iteration 237: loss = 206.03997709794797\n",
      "iteration 238: loss = 206.0392894676764\n",
      "iteration 239: loss = 206.03860802669675\n",
      "iteration 240: loss = 206.03793269400413\n",
      "iteration 241: loss = 206.03726338998314\n",
      "iteration 242: loss = 206.03660003637705\n",
      "iteration 243: loss = 206.03594255626106\n",
      "iteration 244: loss = 206.0352908740128\n",
      "iteration 245: loss = 206.0346449152859\n",
      "iteration 246: loss = 206.03400460698379\n",
      "iteration 247: loss = 206.03336987723205\n",
      "iteration 248: loss = 206.03274065535538\n",
      "iteration 249: loss = 206.0321168718515\n",
      "iteration 250: loss = 206.03149845836757\n",
      "iteration 251: loss = 206.03088534767616\n",
      "iteration 252: loss = 206.0302774736531\n",
      "iteration 253: loss = 206.02967477125478\n",
      "iteration 254: loss = 206.02907717649634\n",
      "iteration 255: loss = 206.0284846264304\n",
      "iteration 256: loss = 206.0278970591265\n",
      "iteration 257: loss = 206.02731441365023\n",
      "iteration 258: loss = 206.02673663004387\n",
      "iteration 259: loss = 206.02616364930734\n",
      "iteration 260: loss = 206.02559541337888\n",
      "iteration 261: loss = 206.0250318651161\n",
      "iteration 262: loss = 206.02447294827937\n",
      "iteration 263: loss = 206.02391860751308\n",
      "iteration 264: loss = 206.02336878832864\n",
      "iteration 265: loss = 206.02282343708856\n",
      "iteration 266: loss = 206.02228250098815\n",
      "iteration 267: loss = 206.021745928042\n",
      "iteration 268: loss = 206.02121366706632\n",
      "iteration 269: loss = 206.02068566766445\n",
      "iteration 270: loss = 206.0201618802119\n",
      "iteration 271: loss = 206.01964225584177\n",
      "iteration 272: loss = 206.01912674642972\n",
      "iteration 273: loss = 206.0186153045817\n",
      "iteration 274: loss = 206.01810788361814\n",
      "iteration 275: loss = 206.01760443756254\n",
      "iteration 276: loss = 206.01710492112676\n",
      "iteration 277: loss = 206.01660928969974\n",
      "iteration 278: loss = 206.0161174993341\n",
      "iteration 279: loss = 206.0156295067339\n",
      "iteration 280: loss = 206.01514526924333\n",
      "iteration 281: loss = 206.01466474483442\n",
      "iteration 282: loss = 206.01418789209612\n",
      "iteration 283: loss = 206.01371467022247\n",
      "iteration 284: loss = 206.01324503900213\n",
      "iteration 285: loss = 206.01277895880793\n",
      "iteration 286: loss = 206.0123163905855\n",
      "iteration 287: loss = 206.01185729584392\n",
      "iteration 288: loss = 206.0114016366451\n",
      "iteration 289: loss = 206.01094937559478\n",
      "iteration 290: loss = 206.0105004758312\n",
      "iteration 291: loss = 206.0100549010183\n",
      "iteration 292: loss = 206.00961261533297\n",
      "iteration 293: loss = 206.0091735834604\n",
      "iteration 294: loss = 206.00873777058112\n",
      "iteration 295: loss = 206.00830514236495\n",
      "iteration 296: loss = 206.00787566496155\n",
      "iteration 297: loss = 206.0074493049924\n",
      "iteration 298: loss = 206.00702602954274\n",
      "iteration 299: loss = 206.00660580615263\n",
      "iteration 300: loss = 206.0061886028109\n",
      "iteration 301: loss = 206.00577438794667\n",
      "iteration 302: loss = 206.00536313042016\n",
      "iteration 303: loss = 206.00495479951908\n",
      "iteration 304: loss = 206.00454936494785\n",
      "iteration 305: loss = 206.00414679682308\n",
      "iteration 306: loss = 206.0037470656646\n",
      "iteration 307: loss = 206.0033501423907\n",
      "iteration 308: loss = 206.00295599830991\n",
      "iteration 309: loss = 206.00256460511505\n",
      "iteration 310: loss = 206.00217593487784\n",
      "iteration 311: loss = 206.00178996004092\n",
      "iteration 312: loss = 206.00140665341274\n",
      "iteration 313: loss = 206.00102598816122\n",
      "iteration 314: loss = 206.0006479378092\n",
      "iteration 315: loss = 206.00027247622583\n",
      "iteration 316: loss = 205.99989957762398\n",
      "iteration 317: loss = 205.99952921655233\n",
      "iteration 318: loss = 205.99916136789122\n",
      "iteration 319: loss = 205.99879600684775\n",
      "iteration 320: loss = 205.99843310894906\n",
      "iteration 321: loss = 205.99807265003776\n",
      "iteration 322: loss = 205.99771460626775\n",
      "iteration 323: loss = 205.99735895409827\n",
      "iteration 324: loss = 205.99700567028938\n",
      "iteration 325: loss = 205.99665473189765\n",
      "iteration 326: loss = 205.99630611627012\n",
      "iteration 327: loss = 205.99595980104152\n",
      "iteration 328: loss = 205.9956157641289\n",
      "iteration 329: loss = 205.99527398372686\n",
      "iteration 330: loss = 205.9949344383043\n",
      "iteration 331: loss = 205.99459710659843\n",
      "iteration 332: loss = 205.99426196761237\n",
      "iteration 333: loss = 205.99392900061034\n",
      "iteration 334: loss = 205.99359818511408\n",
      "iteration 335: loss = 205.99326950089682\n",
      "iteration 336: loss = 205.99294292798305\n",
      "iteration 337: loss = 205.99261844664215\n",
      "iteration 338: loss = 205.9922960373846\n",
      "iteration 339: loss = 205.99197568095948\n",
      "iteration 340: loss = 205.99165735835027\n",
      "iteration 341: loss = 205.99134105077184\n",
      "iteration 342: loss = 205.99102673966576\n",
      "iteration 343: loss = 205.9907144066987\n",
      "iteration 344: loss = 205.99040403375676\n",
      "iteration 345: loss = 205.99009560294445\n",
      "iteration 346: loss = 205.9897890965807\n",
      "iteration 347: loss = 205.9894844971947\n",
      "iteration 348: loss = 205.98918178752413\n",
      "iteration 349: loss = 205.9888809505112\n",
      "iteration 350: loss = 205.9885819693003\n",
      "iteration 351: loss = 205.98828482723457\n",
      "iteration 352: loss = 205.98798950785329\n",
      "iteration 353: loss = 205.98769599488864\n",
      "iteration 354: loss = 205.9874042722637\n",
      "iteration 355: loss = 205.98711432408848\n",
      "iteration 356: loss = 205.9868261346584\n",
      "iteration 357: loss = 205.9865396884504\n",
      "iteration 358: loss = 205.98625497012242\n",
      "iteration 359: loss = 205.98597196450828\n",
      "iteration 360: loss = 205.98569065661596\n",
      "iteration 361: loss = 205.985411031627\n",
      "iteration 362: loss = 205.9851330748914\n",
      "iteration 363: loss = 205.9848567719263\n",
      "iteration 364: loss = 205.9845821084145\n",
      "iteration 365: loss = 205.98430907020062\n",
      "iteration 366: loss = 205.9840376432896\n",
      "iteration 367: loss = 205.9837678138447\n",
      "iteration 368: loss = 205.9834995681849\n",
      "iteration 369: loss = 205.98323289278235\n",
      "iteration 370: loss = 205.98296777426125\n",
      "iteration 371: loss = 205.98270419939453\n",
      "iteration 372: loss = 205.98244215510312\n",
      "iteration 373: loss = 205.9821816284528\n",
      "iteration 374: loss = 205.98192260665328\n",
      "iteration 375: loss = 205.98166507705432\n",
      "iteration 376: loss = 205.9814090271467\n",
      "iteration 377: loss = 205.9811544445574\n",
      "iteration 378: loss = 205.98090131704936\n",
      "iteration 379: loss = 205.98064963252006\n",
      "iteration 380: loss = 205.9803993789977\n",
      "iteration 381: loss = 205.98015054464153\n",
      "iteration 382: loss = 205.97990311773958\n",
      "iteration 383: loss = 205.9796570867055\n",
      "iteration 384: loss = 205.9794124400788\n",
      "iteration 385: loss = 205.97916916652227\n",
      "iteration 386: loss = 205.97892725481958\n",
      "iteration 387: loss = 205.9786866938762\n",
      "iteration 388: loss = 205.97844747271458\n",
      "iteration 389: loss = 205.97820958047416\n",
      "iteration 390: loss = 205.9779730064113\n",
      "iteration 391: loss = 205.97773773989468\n",
      "iteration 392: loss = 205.97750377040566\n",
      "iteration 393: loss = 205.97727108753682\n",
      "iteration 394: loss = 205.97703968099086\n",
      "iteration 395: loss = 205.97680954057702\n",
      "iteration 396: loss = 205.97658065621266\n",
      "iteration 397: loss = 205.97635301791925\n",
      "iteration 398: loss = 205.9761266158229\n",
      "iteration 399: loss = 205.9759014401523\n"
     ]
    }
   ],
   "source": [
    "# train the neural network\n",
    "def train(X, y, num_iterations, learning_rate):\n",
    "    # initialize the weights and biases\n",
    "    parameters = initialize_parameters(input_size, 512, output_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # forward propagation\n",
    "        A3, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = binary_cross_entropy_loss(A3, y)\n",
    "        \n",
    "        # backward propagation\n",
    "        gradients = backward_propagation(parameters, cache, X, y)\n",
    "        \n",
    "        # update the parameters\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        # if i % 1000:\n",
    "        print(f\"iteration {i}: loss = {loss}\")\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = train(train_labels, train_images, num_iterations=400, learning_rate= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, label):\n",
    "    # Prepare input data (reshape label into a column vector)\n",
    "    \n",
    "    # Perform forward propagation to get output from the neural network\n",
    "    output, cache = forward_propagation(label, parameters)\n",
    "    \n",
    "    # Reshape the output into a 28x28 image\n",
    "    image = output.reshape((28, 28))\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Example usage:\n",
    "label = np.array([[2]])  # Assuming train_labels contains the label you want to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_image = predict(parameters, label)\n",
    "\n",
    "# Display the predicted image using matplotlib\n",
    "plt.imshow(predicted_image, cmap='gray')\n",
    "plt.title(\"Predicted Image\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
